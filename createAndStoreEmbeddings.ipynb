{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def chunk_text(text, chunk_size=1024, overlap_ratio=0.1):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "    overlap = int(chunk_size * overlap_ratio)\n",
    "\n",
    "    while start < text_length:\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        start += chunk_size - overlap  # move ahead keeping 10% overlap\n",
    "\n",
    "    return chunks\n",
    "\n",
    "all_chunks = []\n",
    "\n",
    "for filename in os.listdir(\"data\"):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        with open(f\"data/{filename}\", \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "            chunks = chunk_text(text)\n",
    "            all_chunks.extend(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"dataChunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "def getEmbeddings(payload):\n",
    "    API_URL = \"https://router.huggingface.co/hf-inference/models/BAAI/bge-base-en-v1.5/pipeline/feature-extraction\"\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\",\n",
    "    }\n",
    "    response = requests.post(API_URL, headers=headers, json={\"inputs\": payload})\n",
    "    return response.json()\n",
    "\n",
    "embeddings = []\n",
    "batch_size = 32\n",
    "\n",
    "for i in range(0, len(all_chunks), batch_size):\n",
    "    batch = all_chunks[i:i + batch_size]\n",
    "    batch_emb = getEmbeddings(batch)\n",
    "    embeddings.extend(batch_emb)\n",
    "    print(f\"embedding for batch {i} done. Chunks processed : {(i + batch_size) - i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(embeddings[0]), len(embeddings)) #599 embedding made for each chunk each with dimension 768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Combine them in a list of dicts (simple, readable)\n",
    "data = [{\"text\": text, \"embedding\": emb} for text, emb in zip(all_chunks, embeddings)]\n",
    "\n",
    "# Save to a file\n",
    "with open(\"dataEmbeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Load saved data\n",
    "with open(\"dataEmbeddings.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "# Separate text and vectors\n",
    "texts = [d[\"text\"] for d in data]\n",
    "embeddings = np.array([d[\"embedding\"] for d in data]).astype(\"float32\")\n",
    "\n",
    "print(embeddings.shape)\n",
    "\n",
    "d = embeddings.shape[1]\n",
    "\n",
    "faiss.normalize_L2(embeddings)\n",
    "\n",
    "quantizer = faiss.IndexFlatIP(d)\n",
    "# index = faiss.IndexIVFPQ(quantizer, d, nlist=256, m=8, nbits=4)\n",
    "index = faiss.IndexIVFPQ(quantizer, d, 256, 8, 4, faiss.METRIC_INNER_PRODUCT)\n",
    "\n",
    "# nlist - groups all the vectors into nlist clusters -> Higher nlist = better accuracy but more memory and slower training.\n",
    "# m (pq_m) - Each embedding vector (say 384-D) is split into pq_m smaller parts. Example: 384 dimensions → 8 parts → each subvector = 48-D.\n",
    "# pq_bits - Defines how many bits FAISS uses to represent each subvector after quantization. If you set pq_bits = 10 and pq_m = 8, then each vector uses 8 × 10 = 80 bits = 10 bytes of storage.\n",
    "\n",
    "\n",
    "# A bit of debugging shows nx==111 (good) and k==256 (1 << nbits in ProductQuantizer::set_derived_values). I can fix the issue by setting the number of bits per subvector to 4, instead of 8, as in:\n",
    "\n",
    "index.train(embeddings)\n",
    "index.add(embeddings)\n",
    "faiss.write_index(index, \"dataIndexed.faiss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_k(index, q_vec, k=40, similarity_threshold=0.20):\n",
    "    index.nprobe = 8\n",
    "    faiss.normalize_L2(q_vec)\n",
    "    D, I = index.search(q_vec, k)     # D: distances (if normalized embeddings, distance is 2 - 2*cos); depends on index\n",
    "    # If normalized and using inner product: D are distances; if using cosine directly compute via dot\n",
    "    # Here assume we have cosine scores precomputed; otherwise convert appropriately\n",
    "    # Convert distances to cosine similarity if needed\n",
    "    # Example: if index uses IndexFlatIP (inner product) and vectors normalized, D are cosines\n",
    "    sims = D[0]   # array of similarities or distances depending on index setup\n",
    "    idxs = I[0]\n",
    "    # Keep only those above threshold\n",
    "    print( \"Before :\", len(I[0]))\n",
    "    candidates = []\n",
    "    for sim, idx in zip(sims, idxs):\n",
    "        print(sim)\n",
    "        if sim >= similarity_threshold:\n",
    "            candidates.append((idx, sim))\n",
    "    print(\"After :\", len(candidates))\n",
    "    return candidates  # list of (index, similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = getEmbeddings(\"history of car?\")\n",
    "# I → gives the indices of the top-k most similar vectors (your chunks).\n",
    "# D → gives the distance values (how far or close each match is).\n",
    "\n",
    "index.nprobe = 8   # search in 8 clusters\n",
    "D, I = index.search(query_vector, k=40)\n",
    "# here k is - number of nearest neighbors (chunks) you want FAISS to return for each query. This is not searching, just controlling the results return and mainly affects output size\n",
    "# for idx in I[0]:\n",
    "#     print(all_chunks[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = getEmbeddings(\"what are cars?\")\n",
    "retrivedChunksList = retrieve_top_k(index, np.array(query_vector, dtype=np.float32).reshape(1, -1), 20, 0.55)\n",
    "retrivedChunks = [(all_chunks[k[0]], k[1]) for k in retrivedChunksList]\n",
    "textPart = [k[0] for k in retrivedChunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n\".join(textPart)\n",
    "print(context)\n",
    "\n",
    "with open(\"testData.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(query_vector)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
